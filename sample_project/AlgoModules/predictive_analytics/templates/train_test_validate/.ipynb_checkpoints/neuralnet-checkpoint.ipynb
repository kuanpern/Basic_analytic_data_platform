{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network Training/Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: using scikit-learn package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### === import libraries ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_n = 5  # grand repeat number\n",
    "n_cv     = 10 # number of cross-validation parts\n",
    "n_process = 5 # parallel computing for different C.V."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_dir = working_dir+'/vars/'\n",
    "\n",
    "# select latest dated file (track record)\n",
    "cmd = 'ls {data_source_dir}/DF.*.pickle'.format(data_source_dir=data_source_dir)\n",
    "input_filepath = commands.getoutput(cmd).splitlines()[-1]\n",
    "__nb_logger.info('reading file: %s' % input_filepath)\n",
    "DF = pickle.load(open(input_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot transformation\n",
    "DF['is_feature_05'] = DF['feature_05'] == 'default_value'\n",
    "DF['is_feature_06'] = DF['feature_06'] == 'default_value'\n",
    "DF['is_feature_07'] = DF['feature_07'] == 'default_value'\n",
    "\n",
    "one_hots = ['is_feature_05', 'is_feature_06', 'is_feature_07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_features = ['__label', \n",
    "  'feature_01', 'feature_02', 'feature_03', 'feature_04',\n",
    "  'is_feature_05', 'is_feature_06', 'is_feature_07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only columns with selected feature\n",
    "DF = DF[sel_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation on one-hot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = {True: 0.95, False: 0.05}\n",
    "for colname in one_hots:\n",
    "    DF[colname] = map(ds.get, DF[colname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = int((len(DF.columns)-1) * 1.5)\n",
    "b = int(a / 2)\n",
    "ml_config = (a, b, 2)\n",
    "print 'ANN configurations:', ml_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation: separate into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(ml_config, box_train, box_test, label_col = '__label', n_thr_cuts=100, iter_label = None, iter_min = 50):\n",
    "    # read training and testing set\n",
    "    X_train = deepcopy(box_train)\n",
    "    Y_train = X_train.pop(label_col)\n",
    "    X_test  = deepcopy(box_test)\n",
    "    Y_test  = X_test .pop(label_col)\n",
    "\n",
    "    # separate one-hot and non-one-hot\n",
    "    print 'Scaling ... '\n",
    "    p = [_ for _ in X_train if not(_.startswith('is'))]\n",
    "    q = [_ for _ in X_train if     _.startswith('is') ]\n",
    "\n",
    "    btp_train = X_train[p]; btq_train = X_train[q]\n",
    "    btp_test  = X_test [p]; btq_test  = X_test [q]\n",
    "\n",
    "    # standard scaling fitting\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(btp_train)\n",
    "    btp_train = scaler.transform(btp_train)\n",
    "    btp_test  = scaler.transform(btp_test )\n",
    "    \n",
    "    # convert to list type\n",
    "    btp_train = map(list, btp_train); btq_train = map(list, btq_train.values)\n",
    "    btp_test  = map(list, btp_test ); btq_test  = map(list, btq_test .values)\n",
    "\n",
    "    # combine back after separated scaling\n",
    "    for i in range(len(btp_train)):\n",
    "        btp_train[i].extend(list(btq_train[i]))\n",
    "    # end for\n",
    "    for i in range(len(btq_test )):\n",
    "        btp_test [i].extend(list(btq_test [i]))\n",
    "    # end for\n",
    "    X_train = np.array(btp_train)\n",
    "    X_test  = np.array(btp_test)\n",
    "    \n",
    "    # formatting Y_train to scalar value\n",
    "    Y_train = Y_train.values\n",
    "\n",
    "    print 'Starting ANN ...'\n",
    "    # ANN initialization\n",
    "    mlp = MLPClassifier(hidden_layer_sizes = ml_config, warm_start=True)\n",
    "\n",
    "    # ANN fitting\n",
    "    # ensure the proportion is roughly 50%:50%\n",
    "    print 'make balanced datasets ...'\n",
    "    balanced_data = sample_balance_datasets(X_train, Y_train)\n",
    "    print 'started training ...'\n",
    "    \n",
    "    print 'max-iter:',\n",
    "    mlp_iter_record = [iter_min]\n",
    "    for j in range(len(balanced_data)):\n",
    "        x_train, y_train = balanced_data[j]\n",
    "        \n",
    "        while True:\n",
    "            det_repeat = False\n",
    "            mlp.fit(x_train, y_train)\n",
    "            \n",
    "            if j == 0:\n",
    "                this_n_iter = mlp.n_iter_\n",
    "            else:\n",
    "                this_n_iter = mlp.n_iter_ - mlp_iter_record[-1]\n",
    "            # end if\n",
    "            \n",
    "            if this_n_iter < max(iter_min, mlp_iter_record[0]):\n",
    "                det_repeat = True\n",
    "\n",
    "            if det_repeat == False:\n",
    "                if j == 0:\n",
    "                    mlp_iter_record = [mlp.n_iter_]\n",
    "                else:\n",
    "                    mlp_iter_record.append(mlp.n_iter_)\n",
    "                # end if\n",
    "                print mlp_iter_record[-1],\n",
    "                break\n",
    "            # end if\n",
    "        # end while\n",
    "    # end for\n",
    "    print\n",
    "\n",
    "    # logging training and testing accuracy\n",
    "    print '(training) iteration:', iter_label, '| accuracy:', mlp.score(X_train, Y_train)\n",
    "    print '(testing ) iteration:', iter_label, '| accuracy:', mlp.score(X_test , Y_test )\n",
    "\n",
    "    # scoring the testing set by ranking\n",
    "    probs = mlp.predict_proba(X_test)\n",
    "    probs = np.array(zip(*probs)[0])\n",
    "    ids = box_test.index.values\n",
    "    # record the probs\n",
    "    test_scores = dict(zip(ids, probs))\n",
    "\n",
    "    # full statistical assessment with varying threshold\n",
    "    thrs = list(set(map(float, probs)))\n",
    "    thrs = np.percentile(thrs, np.arange(n_thr_cuts))\n",
    "\n",
    "    Cs = {}\n",
    "    for thr in thrs:\n",
    "        predictions = probs < thr\n",
    "        labels      = Y_test\n",
    "\n",
    "        confusion_matrix = compute_confusion_matrix(predictions, labels)\n",
    "        out = binary_classification_assessment(confusion_matrix)\n",
    "\n",
    "        Cs[thr] = out\n",
    "    # end for\n",
    "\n",
    "    return {'model': {'mlp': mlp, 'scaler': scaler}, 'results': test_scores, 'assessments': Cs}\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare memory for MLPs\n",
    "MLPs        = []\n",
    "CV_Recs     = []\n",
    "test_scores = []\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel computing version\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def run_ml_epoch(input_vars):\n",
    "    ml_config, box_train, box_test, label_col, n_thr_cuts, iter_label = input_vars\n",
    "    return run_epoch(ml_config, box_train, box_test, label_col, n_thr_cuts, iter_label)\n",
    "# end def\n",
    "\n",
    "for _episode in range(repeat_n):\n",
    "    # separate into training and testing set\n",
    "    print 'make cross-validation datasets ...'\n",
    "    boxes = construct_CV_datasets(DF, '__label', n_cv)\n",
    "    \n",
    "    inputs  = [(ml_config, boxes[i]['training'], boxes[i]['testing'], '__label', 100, i+1) for i in range(n_cv)]\n",
    "    pool = Pool(processes=n_process)\n",
    "    outputs = pool.map(run_ml_epoch, inputs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    " \n",
    "    for output in outputs:\n",
    "        MLPs       .append(output['model'])\n",
    "        test_scores.append(output['results'])\n",
    "        CV_Recs    .append(output['assessments'])\n",
    "    # end for\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment: ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC\n",
    "R = []\n",
    "[R.extend(_.values()) for _ in CV_Recs];\n",
    "R = pd.DataFrame(R)\n",
    "plt.scatter(*zip(*R[['FPR', 'TPR']].values), s = 0.5)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], ls = '--', color = 'red')\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.gcf().set_size_inches(9, 6)\n",
    "plt.title('Receiver operating characteristic curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_timestamp = int(utc_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### append scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = defauldict(lambda: [])\n",
    "for _scores in test_scores:\n",
    "    for key, val in _scores.iteritems():\n",
    "        final_scores[key].append(val)\n",
    "    # end for\n",
    "# end for\n",
    "for key, vals in final_scores.iteritems():\n",
    "    final_scores[key] = np.mean(vals)\n",
    "# end for\n",
    "DF['score'] = map(final_scores.get, DF.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify output\n",
    "outfilename = working_dir + '/vars/DF.%s.pickle' % _timestamp\n",
    "__nb_logger.info('write to output: %s' % outfilename)\n",
    "pickle.dump(DF, open(outfilename, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assessment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify output\n",
    "outfilename = working_dir + '/results/assessment_records.%s.xlsx' % _timestamp\n",
    "__nb_logger.info('write to output: %s' % outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFs = []\n",
    "for part in CV_Recs:\n",
    "    thrs    = part.keys()\n",
    "    results = part.values()\n",
    "    _df = pd.DataFrame(part.values())\n",
    "    _df['threshold'] = thrs\n",
    "    DFs.append(_df)\n",
    "# end for\n",
    "_df = pd.concat(DFs, axis=0)\n",
    "_df.sort_values(by=['threshold', 'ACC'])\n",
    "_df.set_index('threshold', inplace=True)\n",
    "_df.to_excel(outfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify output\n",
    "outfilename = working_dir + '/models/MLP.%s.pickle' % _timestamp\n",
    "__nb_logger.info('write to output: %s' % outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(MLPs, open(outfilename, 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
